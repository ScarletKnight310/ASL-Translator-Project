{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45866f74",
   "metadata": {},
   "source": [
    "# I. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a3915",
   "metadata": {},
   "source": [
    " Tensorflow 2.8.0 / Install core with GPU support (also 2.8.0) / pure windows / https://www.tensorflow.org/install/source_windows / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71a4cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.8.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\tensorflow-2.8.0-py3.9-win-amd64.egg (2.8.0)\n",
      "Requirement already satisfied: tensorflow-gpu==2.8.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (0.8.9.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\matplotlib-3.5.1-py3.9-win-amd64.egg (3.5.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\absl_py-1.0.0-py3.9.egg (from tensorflow==2.8.0) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (2.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.44.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (3.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.1.2)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\keras-2.8.0-py3.9.egg (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.22.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\protobuf-3.20.0rc2-py3.9-win-amd64.egg (from tensorflow==2.8.0) (3.20.0rc2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (56.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\tensorflow_io_gcs_filesystem-0.24.0-py3.9-win-amd64.egg (from tensorflow==2.8.0) (0.24.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.1.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\typing_extensions-4.1.1-py3.9.egg (from tensorflow==2.8.0) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorflow==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from mediapipe) (4.5.5.64)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\cycler-0.11.0-py3.9.egg (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\fonttools-4.31.1-py3.9.egg (from matplotlib) (4.31.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\kiwisolver-1.4.0-py3.9-win-amd64.egg (from matplotlib) (1.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\packaging-21.3-py3.9.egg (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\pillow-9.0.1-py3.9-win-amd64.egg (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\pyparsing-3.0.7-py3.9.egg (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\requests-2.27.1-py3.9.egg (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.6)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages\\scipy-1.8.0-py3.9-win-amd64.egg (from scikit-learn->sklearn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kashod cagnolatti\\desktop\\projects\\python\\asl-translator-project\\src\\asltenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.8.0 tensorflow-gpu==2.8.0 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074accc",
   "metadata": {},
   "source": [
    "Import used libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "756dcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # *may not need this*\n",
    "import time # time of os / for delays\n",
    "import cv2 # get data from webcam\n",
    "import mediapipe as mp # mediapipe extra, analyze data\n",
    "import numpy as np # number related\n",
    "from matplotlib import pyplot as plt # plot / draw / organize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00cddd",
   "metadata": {},
   "source": [
    "Docs\n",
    "- CV2\n",
    "- Mediapipe https://google.github.io/mediapipe/\n",
    "- Numpy\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19361bbd",
   "metadata": {},
   "source": [
    "# II. Prep Data Capture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "490e6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255c32b",
   "metadata": {},
   "source": [
    "MP Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f89d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02b64a",
   "metadata": {},
   "source": [
    "Draw Landmarks (for testing) / for syntax change https://github.com/google/mediapipe/issues/2448 /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "facfedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "215fcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f7a07",
   "metadata": {},
   "source": [
    "Confidence Inti / Sep. for future collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "702a5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectConfi = 0.5\n",
    "TrackingConfi = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfedde",
   "metadata": {},
   "source": [
    "# III. Extract Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250040c",
   "metadata": {},
   "source": [
    "Hold Raw position data / Loops, collects, and handles data / for passing data into model will flatten and merge / will return array of zeros if data can't be found / will be used in part V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12b47426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose_data = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face_data = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh_data = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh_data = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose_data, face_data, lh_data, rh_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cab68f",
   "metadata": {},
   "source": [
    "# IV. Organize Data, Pre-Capture Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a8bcf",
   "metadata": {},
   "source": [
    "Setup/Update ASL Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f09a3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('ASL System Library') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "\n",
    "# 'n' videos woTrth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 'n' frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d137a",
   "metadata": {},
   "source": [
    "Setup/Update Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf8aa7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d47226",
   "metadata": {},
   "source": [
    "# V. Capture Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03092c6",
   "metadata": {},
   "source": [
    "Iterates thru whole library / For auto collection / prepare before running, sign -> wait -> sign / follow the terminal for current sign / press 'Q' to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2380c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=DetectConfi, min_tracking_confidence=TrackingConfi) as holistic:\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for \\'{}\\' Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57161bff",
   "metadata": {},
   "source": [
    "#  VI. Process Collected Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9c374",
   "metadata": {},
   "source": [
    "Import extra dependencies / for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ccb84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db919c2d",
   "metadata": {},
   "source": [
    "Create dictionary based on current ASL signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82d21fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25102290",
   "metadata": {},
   "source": [
    "Check contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eac26bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'K': 10,\n",
       " 'L': 11,\n",
       " 'M': 12,\n",
       " 'N': 13,\n",
       " 'O': 14,\n",
       " 'P': 15,\n",
       " 'Q': 16,\n",
       " 'R': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'U': 20,\n",
       " 'V': 21,\n",
       " 'W': 22,\n",
       " 'X': 23,\n",
       " 'Y': 24,\n",
       " 'Z': 25}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245173a",
   "metadata": {},
   "source": [
    "Merge all Data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b85a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b98577",
   "metadata": {},
   "source": [
    "prep for model, split for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9049978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2841859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "045717be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05) # split for testing/ training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627e5ef",
   "metadata": {},
   "source": [
    "# VII. Build and Test LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297d403",
   "metadata": {},
   "source": [
    "Import used tensorflow libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2230e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Seq Neu Net\n",
    "from tensorflow.keras.models import Sequential\n",
    "# LSTM Layer, for action detection\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "# For logging and checking Model status\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eb63e9",
   "metadata": {},
   "source": [
    "For viewing training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9808dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131680d",
   "metadata": {},
   "source": [
    "Build Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92fb5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d79316",
   "metadata": {},
   "source": [
    "Compile and Train Model / Check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b51e70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "215bf290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "24/24 [==============================] - 3s 55ms/step - loss: 6.7636 - categorical_accuracy: 0.0310\n",
      "Epoch 2/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 35.0414 - categorical_accuracy: 0.0324\n",
      "Epoch 3/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 12.0043 - categorical_accuracy: 0.0364\n",
      "Epoch 4/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 5.0922 - categorical_accuracy: 0.0378\n",
      "Epoch 5/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.6508 - categorical_accuracy: 0.0445\n",
      "Epoch 6/2000\n",
      "24/24 [==============================] - 1s 53ms/step - loss: 3.3846 - categorical_accuracy: 0.0459\n",
      "Epoch 7/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.6027 - categorical_accuracy: 0.0337\n",
      "Epoch 8/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.4728 - categorical_accuracy: 0.0351\n",
      "Epoch 9/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.7901 - categorical_accuracy: 0.0418\n",
      "Epoch 10/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.5518 - categorical_accuracy: 0.0526\n",
      "Epoch 11/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2733 - categorical_accuracy: 0.0270\n",
      "Epoch 12/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2602 - categorical_accuracy: 0.0351\n",
      "Epoch 13/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2602 - categorical_accuracy: 0.0324\n",
      "Epoch 14/2000\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 3.2615 - categorical_accuracy: 0.0391\n",
      "Epoch 15/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2580 - categorical_accuracy: 0.0391\n",
      "Epoch 16/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2570 - categorical_accuracy: 0.0391\n",
      "Epoch 17/2000\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 3.2610 - categorical_accuracy: 0.0405\n",
      "Epoch 18/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2595 - categorical_accuracy: 0.0364\n",
      "Epoch 19/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2587 - categorical_accuracy: 0.0418\n",
      "Epoch 20/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2594 - categorical_accuracy: 0.0405\n",
      "Epoch 21/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2582 - categorical_accuracy: 0.0405\n",
      "Epoch 22/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.3091 - categorical_accuracy: 0.0351\n",
      "Epoch 23/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2566 - categorical_accuracy: 0.0405\n",
      "Epoch 24/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2562 - categorical_accuracy: 0.0445\n",
      "Epoch 25/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2553 - categorical_accuracy: 0.0418\n",
      "Epoch 26/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2547 - categorical_accuracy: 0.0418\n",
      "Epoch 27/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2543 - categorical_accuracy: 0.0378\n",
      "Epoch 28/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2540 - categorical_accuracy: 0.0418\n",
      "Epoch 29/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2535 - categorical_accuracy: 0.0418\n",
      "Epoch 30/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2527 - categorical_accuracy: 0.0378\n",
      "Epoch 31/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2520 - categorical_accuracy: 0.0432\n",
      "Epoch 32/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2516 - categorical_accuracy: 0.0432\n",
      "Epoch 33/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2512 - categorical_accuracy: 0.0378\n",
      "Epoch 34/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2508 - categorical_accuracy: 0.0445\n",
      "Epoch 35/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2504 - categorical_accuracy: 0.0432\n",
      "Epoch 36/2000\n",
      "24/24 [==============================] - 1s 54ms/step - loss: 3.2500 - categorical_accuracy: 0.0432\n",
      "Epoch 37/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2498 - categorical_accuracy: 0.0432\n",
      "Epoch 38/2000\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 3.2496 - categorical_accuracy: 0.0432\n",
      "Epoch 39/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2491 - categorical_accuracy: 0.0432\n",
      "Epoch 40/2000\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 3.2488 - categorical_accuracy: 0.0445\n",
      "Epoch 41/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2484 - categorical_accuracy: 0.0445\n",
      "Epoch 42/2000\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 3.2485 - categorical_accuracy: 0.0418\n",
      "Epoch 43/2000\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 3.2478 - categorical_accuracy: 0.0432\n",
      "Epoch 44/2000\n",
      "20/24 [========================>.....] - ETA: 0s - loss: 3.2451 - categorical_accuracy: 0.0375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\keras-2.8.0-py3.9.egg\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\keras-2.8.0-py3.9.egg\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Desktop\\Projects\\Python\\ASL-Translator-Project\\src\\ASLTenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48e0c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 26)                858       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 597,434\n",
      "Trainable params: 597,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13007c",
   "metadata": {},
   "source": [
    "# VIII. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5347fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ASL_DLModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9062d",
   "metadata": {},
   "source": [
    "# IX. Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e5ed9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ad15a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "542ef126",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1380082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[36.,  0.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[36.,  0.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[36.,  0.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[37.,  1.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[36.,  0.],\n",
       "        [ 3.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[35.,  0.],\n",
       "        [ 4.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[37.,  0.],\n",
       "        [ 2.,  0.]],\n",
       "\n",
       "       [[38.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[ 1., 38.],\n",
       "        [ 0.,  0.]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab0ef31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829b71c",
   "metadata": {},
   "source": [
    "# X. Real Time Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0d818",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a2ed14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b289a14",
   "metadata": {},
   "source": [
    "Test with Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74887f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "Z\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m         sentence \u001b[38;5;241m=\u001b[39m sentence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:]\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Viz probabilities\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mprob_viz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(image, (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m40\u001b[39m), (\u001b[38;5;241m245\u001b[39m, \u001b[38;5;241m117\u001b[39m, \u001b[38;5;241m16\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence), (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m30\u001b[39m), \n\u001b[0;32m     48\u001b[0m                cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mprob_viz\u001b[1;34m(res, actions, input_frame, colors)\u001b[0m\n\u001b[0;32m      3\u001b[0m output_frame \u001b[38;5;241m=\u001b[39m input_frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(res):\n\u001b[1;32m----> 5\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(output_frame, (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), (\u001b[38;5;28mint\u001b[39m(prob\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m), \u001b[38;5;241m90\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), \u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(output_frame, actions[num], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m85\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_frame\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c6157",
   "metadata": {},
   "source": [
    "Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[np.argmax(res)] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_sequences,30,1662)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3603b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.expand_dims(X_test[0], axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASLTenv",
   "language": "python",
   "name": "asltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
